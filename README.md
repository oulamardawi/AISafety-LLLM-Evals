# AISafety-LLLM-Evals
This repository contains practical experiments and exploratory notebooks focused on evaluating large language models (LLMs) under an AI safety lens. It includes methods for detecting hallucinations, measuring semantic uncertainty (e.g., via semantic entropy), and testing model reliability across clinical and high-stakes tasks.

The work is part of my ongoing self-study and research exploration in AI safety, with a focus on robust LLM behavior and safe deployment.
